{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import cntk as C\n",
    "import cntk.tests.test_utils\n",
    "\n",
    "#cntk.tests.test_utils.set_device_from_pytest_env()\n",
    "C.cntk_py.set_fixed_random_seed(1)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim_model = (1, 28, 28)\n",
    "input_dim = 28 * 28\n",
    "num_output_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read a CTF formatted text (as mentioned above) using the CTF deserializer from a file\n",
    "def create_reader(path, is_training, input_dim, num_label_classes):\n",
    "    \n",
    "    ctf = C.io.CTFDeserializer(path, C.io.StreamDefs(\n",
    "          labels=C.io.StreamDef(field='labels', shape=num_label_classes, is_sparse=False),\n",
    "          features=C.io.StreamDef(field='features', shape=input_dim, is_sparse=False)))\n",
    "                          \n",
    "    return C.io.MinibatchSource(ctf,\n",
    "        randomize = is_training, max_sweeps = C.io.INFINITELY_REPEAT if is_training else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory is data\\MNIST\n"
     ]
    }
   ],
   "source": [
    "# Ensure the training and test data is available for this tutorial.\n",
    "# We search in two locations in the toolkit for the cached MNIST data set.\n",
    "\n",
    "data_found=False # A flag to indicate if train/test data found in local cache\n",
    "for data_dir in [os.path.join(\"..\", \"Examples\", \"Image\", \"DataSets\", \"MNIST\"),\n",
    "                 os.path.join(\"data\", \"MNIST\")]:\n",
    "    \n",
    "    train_file=os.path.join(data_dir, \"Train-28x28_cntk_text.txt\")\n",
    "    test_file=os.path.join(data_dir, \"Test-28x28_cntk_text.txt\")\n",
    "    \n",
    "    if os.path.isfile(train_file) and os.path.isfile(test_file):\n",
    "        data_found=True\n",
    "        break\n",
    "        \n",
    "if not data_found:\n",
    "    raise ValueError(\"Please generate the data by completing CNTK 103 Part A\")\n",
    "    \n",
    "print(\"Data directory is {0}\".format(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = C.input_variable(input_dim_model)\n",
    "y = C.input_variable(num_output_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(features):\n",
    "    with C.layers.default_options(init=C.glorot_uniform(), activation=C.relu):\n",
    "        h = features\n",
    "        h = C.layers.Convolution2D(filter_shape=(5, 5),\n",
    "                                  num_filters=8,\n",
    "                                  strides=(2,2),\n",
    "                                  pad=True, name='first_conv')(h)\n",
    "        h = C.layers.Convolution2D(filter_shape=(5, 5),\n",
    "                                  num_filters=16,\n",
    "                                  strides=(2,2),\n",
    "                                  pad=True, name='second_conv')(h)\n",
    "        r = C.layers.Dense(num_output_classes, activation=None, name='classify')(h)\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Shape of the first convolution layer: (8, 14, 14)\n",
      "Bias value of the last dense layer: [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "z = create_model(x)\n",
    "\n",
    "print(\"Output Shape of the first convolution layer:\", z.first_conv.shape)\n",
    "print(\"Bias value of the last dense layer:\", z.classify.b.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 11274 parameters in 6 parameter tensors.\n"
     ]
    }
   ],
   "source": [
    "C.logging.log_number_of_parameters(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_criterion_function(model, labels):\n",
    "    loss = C.cross_entropy_with_softmax(model, labels)\n",
    "    errs = C.classification_error(model, labels)\n",
    "    return loss, errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a utility function to compute the moving average sum.\n",
    "# A more efficient implementation is possible with np.cumsum() function\n",
    "def moving_average(a, w=5):\n",
    "    if len(a) < w:\n",
    "        return a[:]    # Need to send a copy of the array\n",
    "    return [val if idx < w else sum(a[(idx-w):idx])/w for idx, val in enumerate(a)]\n",
    "\n",
    "\n",
    "# Defines a utility that prints the training progress\n",
    "def print_training_progress(trainer, mb, frequency, verbose=1):\n",
    "    training_loss = \"NA\"\n",
    "    eval_error = \"NA\"\n",
    "\n",
    "    if mb%frequency == 0:\n",
    "        training_loss = trainer.previous_minibatch_loss_average\n",
    "        eval_error = trainer.previous_minibatch_evaluation_average\n",
    "        if verbose: \n",
    "            print (\"Minibatch: {0}, Loss: {1:.4f}, Error: {2:.2f}%\".format(mb, training_loss, eval_error*100))\n",
    "        \n",
    "    return mb, training_loss, eval_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test(train_reader, test_reader, model_func, num_sweeps_to_train_with=10):\n",
    "    \n",
    "    # Instantiate the model function; x is the input (feature) variable \n",
    "    # We will scale the input image pixels within 0-1 range by dividing all input value by 255.\n",
    "    model = model_func(x/255)\n",
    "    \n",
    "    # Instantiate the loss and error function\n",
    "    loss, label_error = create_criterion_function(model, y)\n",
    "    \n",
    "    # Instantiate the trainer object to drive the model training\n",
    "    learning_rate = 0.2\n",
    "    lr_schedule = C.learning_rate_schedule(learning_rate, C.UnitType.minibatch)\n",
    "    #learner = C.sgd(z.parameters, lr_schedule)\n",
    "    m = C.momentum_schedule(0.9)\n",
    "    learner = C.adam(z.parameters, lr_schedule, m)\n",
    "    trainer = C.Trainer(z, (loss, label_error), [learner])\n",
    "    \n",
    "    # Initialize the parameters for the trainer\n",
    "    minibatch_size = 64\n",
    "    num_samples_per_sweep = 60000\n",
    "    num_minibatches_to_train = (num_samples_per_sweep * num_sweeps_to_train_with) / minibatch_size\n",
    "    \n",
    "    # Map the data streams to the input and labels.\n",
    "    input_map={\n",
    "        y  : train_reader.streams.labels,\n",
    "        x  : train_reader.streams.features\n",
    "    } \n",
    "    \n",
    "    # Uncomment below for more detailed logging\n",
    "    training_progress_output_freq = 500\n",
    "     \n",
    "    # Start a timer\n",
    "    start = time.time()\n",
    "\n",
    "    for i in range(0, int(num_minibatches_to_train)):\n",
    "        # Read a mini batch from the training data file\n",
    "        data=train_reader.next_minibatch(minibatch_size, input_map=input_map) \n",
    "        trainer.train_minibatch(data)\n",
    "        print_training_progress(trainer, i, training_progress_output_freq, verbose=1)\n",
    "     \n",
    "    # Print training time\n",
    "    print(\"Training took {:.1f} sec\".format(time.time() - start))\n",
    "    \n",
    "    # Test the model\n",
    "    test_input_map = {\n",
    "        y  : test_reader.streams.labels,\n",
    "        x  : test_reader.streams.features\n",
    "    }\n",
    "\n",
    "    # Test data for trained model\n",
    "    test_minibatch_size = 512\n",
    "    num_samples = 10000\n",
    "    num_minibatches_to_test = num_samples // test_minibatch_size\n",
    "\n",
    "    test_result = 0.0   \n",
    "\n",
    "    for i in range(num_minibatches_to_test):\n",
    "    \n",
    "        # We are loading test data in batches specified by test_minibatch_size\n",
    "        # Each data point in the minibatch is a MNIST digit image of 784 dimensions \n",
    "        # with one pixel per dimension that we will encode / decode with the \n",
    "        # trained model.\n",
    "        data = test_reader.next_minibatch(test_minibatch_size, input_map=test_input_map)\n",
    "        eval_error = trainer.test_minibatch(data)\n",
    "        test_result = test_result + eval_error\n",
    "\n",
    "    # Average of evaluation errors of all test minibatches\n",
    "    print(\"Average test error: {0:.2f}%\".format(test_result*100 / num_minibatches_to_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch: 0, Loss: 2.3093, Error: 89.06%\n",
      "Minibatch: 500, Loss: 0.1887, Error: 9.38%\n",
      "Minibatch: 1000, Loss: 0.0366, Error: 1.56%\n",
      "Minibatch: 1500, Loss: 0.0673, Error: 0.00%\n",
      "Minibatch: 2000, Loss: 0.0058, Error: 0.00%\n",
      "Minibatch: 2500, Loss: 0.0090, Error: 0.00%\n",
      "Minibatch: 3000, Loss: 0.0278, Error: 1.56%\n",
      "Minibatch: 3500, Loss: 0.0427, Error: 1.56%\n",
      "Minibatch: 4000, Loss: 0.0297, Error: 1.56%\n",
      "Minibatch: 4500, Loss: 0.0220, Error: 1.56%\n",
      "Minibatch: 5000, Loss: 0.0032, Error: 0.00%\n",
      "Minibatch: 5500, Loss: 0.0007, Error: 0.00%\n",
      "Minibatch: 6000, Loss: 0.0156, Error: 1.56%\n",
      "Minibatch: 6500, Loss: 0.0191, Error: 1.56%\n",
      "Minibatch: 7000, Loss: 0.0018, Error: 0.00%\n",
      "Minibatch: 7500, Loss: 0.0037, Error: 0.00%\n",
      "Minibatch: 8000, Loss: 0.0002, Error: 0.00%\n",
      "Minibatch: 8500, Loss: 0.0052, Error: 0.00%\n",
      "Minibatch: 9000, Loss: 0.0011, Error: 0.00%\n",
      "Training took 86.1 sec\n",
      "Average test error: 1.39%\n"
     ]
    }
   ],
   "source": [
    "def do_train_test():\n",
    "    global z\n",
    "    z = create_model(x)\n",
    "    reader_train = create_reader(train_file, True, input_dim, num_output_classes)\n",
    "    reader_test = create_reader(test_file, False, input_dim, num_output_classes)\n",
    "    train_test(reader_train, reader_test, z)\n",
    "    \n",
    "do_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = C.softmax(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reader_eval=create_reader(test_file, False, input_dim, num_output_classes)\n",
    "\n",
    "eval_minibatch_size = 25\n",
    "eval_input_map = {x: reader_eval.streams.features, y:reader_eval.streams.labels}\n",
    "\n",
    "data = reader_eval.next_minibatch(eval_minibatch_size, input_map=eval_input_map)\n",
    "\n",
    "img_label = data[y].asarray()\n",
    "img_data = data[x].asarray()\n",
    "\n",
    "img_data = np.reshape(img_data, (eval_minibatch_size, 1, 28, 28))\n",
    "\n",
    "predicted_label_prob = [out.eval(img_data[i]) for i in range(len(img_data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = [np.argmax(predicted_label_prob[i]) for i in range(len(predicted_label_prob))]\n",
    "gtlabel = [np.argmax(img_label[i]) for i in range(len(img_label))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label   : [7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6, 5, 4]\n",
      "Predicted: [7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 8, 4, 9, 6, 6, 5, 4]\n"
     ]
    }
   ],
   "source": [
    "print(\"Label   :\", gtlabel[:25])\n",
    "print(\"Predicted:\", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Label:  1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAABRNJREFUeJzt3a9uFF0cgOEuLaQoEFhA4UhISMCQ4GpBI7AkRXERwA1w\nGSQYFBJCapAEh0CAbHAkpGI/89k5JTvZ/tn3eeyvc2aT8nLE2ZkulsvlFtBz4bQ/AHA6xA9R4oco\n8UOU+CFK/BAlfogSP0SJH6J2Tvh+vk4I67f4lx+y80OU+CFK/BAlfogSP0SJH6LED1HihyjxQ5T4\nIUr8ECV+iBI/RIkfosQPUeKHKPFDlPghSvwQJX6IEj9EiR+ixA9R4oco8UOU+CFK/BAlfogSP0SJ\nH6LED1HihyjxQ5T4IUr8ECV+iBI/RIkfonZO+wOw2d6/fz85e/To0fDaN2/eDOf7+/vD+fb29nBe\nZ+eHKPFDlPghSvwQJX6IEj9EiR+iFsvl8iTvd6I3Y/0ODw+H8zt37kzOfv36Nevef/78Gc4vX748\na/1zbPEvP2TnhyjxQ5T4IUr8ECV+iBI/RHmkl1k+ffo0nM85znvy5Mlwvru7u/La2PkhS/wQJX6I\nEj9EiR+ixA9R4oco5/wM/f37dzh/+fLl2u799OnT4Xyx+KcnV5lg54co8UOU+CFK/BAlfogSP0SJ\nH6K8upuhL1++DOf3799fee2dnfHXTI6OjlZeO86ru4Fp4oco8UOU+CFK/BAlfogSP0R5np+hd+/e\nrW3tvb29ta3N8ez8ECV+iBI/RIkfosQPUeKHKPFDlHN+hj5+/Djr+kuXLk3OXr9+PWtt5rHzQ5T4\nIUr8ECV+iBI/RIkfory6O+7g4GA4f/Dgwaz1r169Ojn7/fv3rLWZ5NXdwDTxQ5T4IUr8ECV+iBI/\nRIkfojzSG3fcn+Cea39/f63rszo7P0SJH6LED1HihyjxQ5T4IUr8EOWcP27uOf/oef2tra2t58+f\nz1qf9bHzQ5T4IUr8ECV+iBI/RIkfosQPUd7bv+E+f/48nD98+HA4P+7fx82bN4fzHz9+DOeshff2\nA9PED1HihyjxQ5T4IUr8ECV+iPI8/4Y7PDwczud+z2Nvb2/W9ZweOz9EiR+ixA9R4oco8UOU+CHK\nUd+Ge/v27azrj3s197Nnz2atz+mx80OU+CFK/BAlfogSP0SJH6LED1Fe3b0Bfv78OTm7cePG8Nrj\nfv+3b98ezr9+/Tqccyq8uhuYJn6IEj9EiR+ixA9R4oco8UOU5/k3wMHBweRs7vc4Hj9+POt6zi47\nP0SJH6LED1HihyjxQ5T4IUr8EOWcfwMc92e4R65duzacv3jxYuW1Odvs/BAlfogSP0SJH6LED1Hi\nhyhHfRvgw4cPK197/fr14fzKlSsrr83ZZueHKPFDlPghSvwQJX6IEj9EiR+inPOfA0dHR8P59+/f\nV157d3d3OL948eLKa3O22fkhSvwQJX6IEj9EiR+ixA9R4oco5/znwIUL4/+j7927Nzn79u3b8Npb\nt26t9Jk4/+z8ECV+iBI/RIkfosQPUeKHKPFDlHP+c2B7e3s4f/Xq1eRssVgMr7179+5Kn4nzz84P\nUeKHKPFDlPghSvwQJX6IEj9ELZbL5Une70RvBlHjL3f8z84PUeKHKPFDlPghSvwQJX6IEj9EiR+i\nxA9R4oco8UOU+CFK/BAlfogSP0SJH6LED1HihyjxQ5T4IUr8ECV+iDrpP9H9T68UBtbPzg9R4oco\n8UOU+CFK/BAlfogSP0SJH6LED1HihyjxQ5T4IUr8ECV+iBI/RIkfosQPUeKHKPFDlPghSvwQJX6I\nEj9E/Qdk0Id4759HOgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1906e588e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_number = 5\n",
    "plt.imshow(img_data[sample_number].reshape(28, 28), cmap=\"gray_r\")\n",
    "plt.axis('off')\n",
    "\n",
    "img_gt, img_pred = gtlabel[sample_number], pred[sample_number]\n",
    "print(\"Image Label: \", img_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch: 0, Loss: 2.3210, Error: 92.19%\n",
      "Minibatch: 500, Loss: 0.1180, Error: 4.69%\n",
      "Minibatch: 1000, Loss: 0.0719, Error: 1.56%\n",
      "Minibatch: 1500, Loss: 0.0631, Error: 1.56%\n",
      "Minibatch: 2000, Loss: 0.0013, Error: 0.00%\n",
      "Minibatch: 2500, Loss: 0.0045, Error: 0.00%\n",
      "Minibatch: 3000, Loss: 0.0022, Error: 0.00%\n",
      "Minibatch: 3500, Loss: 0.0661, Error: 1.56%\n",
      "Minibatch: 4000, Loss: 0.0053, Error: 0.00%\n",
      "Minibatch: 4500, Loss: 0.0038, Error: 0.00%\n",
      "Minibatch: 5000, Loss: 0.0266, Error: 1.56%\n",
      "Minibatch: 5500, Loss: 0.0037, Error: 0.00%\n",
      "Minibatch: 6000, Loss: 0.0436, Error: 1.56%\n",
      "Minibatch: 6500, Loss: 0.0084, Error: 0.00%\n",
      "Minibatch: 7000, Loss: 0.0116, Error: 0.00%\n",
      "Minibatch: 7500, Loss: 0.0034, Error: 0.00%\n",
      "Minibatch: 8000, Loss: 0.0049, Error: 0.00%\n",
      "Minibatch: 8500, Loss: 0.0078, Error: 0.00%\n",
      "Minibatch: 9000, Loss: 0.0010, Error: 0.00%\n",
      "Training took 247.0 sec\n",
      "Average test error: 1.19%\n"
     ]
    }
   ],
   "source": [
    "def create_model(features):\n",
    "    with C.layers.default_options(init = C.layers.glorot_uniform(), activation = C.relu):\n",
    "        h = features\n",
    "        \n",
    "        h = C.layers.Convolution2D(filter_shape=(5, 5),\n",
    "                                  num_filters=8,\n",
    "                                  strides=(1,1),\n",
    "                                  pad=True, name=\"first_conv\")(h)\n",
    "        h = C.layers.MaxPooling(filter_shape=(2,2),\n",
    "                               strides=(2,2), name=\"first_max\")(h)\n",
    "        h = C.layers.Convolution2D(filter_shape=(5, 5),\n",
    "                                  num_filters=16,\n",
    "                                  strides=(1,1),\n",
    "                                  pad=True, name=\"second_conv\")(h)\n",
    "        h = C.layers.MaxPooling(filter_shape=(2,2),\n",
    "                               strides=(2,2), name=\"second_max\")(h)\n",
    "        r = C.layers.Dense(num_output_classes, activation = None, name=\"classify\")(h)\n",
    "        return r\n",
    "    \n",
    "do_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
